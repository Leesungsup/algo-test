# -*- coding: utf-8 -*-
"""연습문제_5_2_부스팅_와인_그리드서치튜닝.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xs7aSYpSv4R01OkcSwQG6JNCDW3Dbcp5
"""

#Step 1. 구글 코랩에 한글 폰트 설정하기

!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf



#Step 2.분석할 데이터가 저장된 파일을 불러와서 변수에 할당합니다.
from google.colab import files
myfile = files.upload()
import io
import pandas as pd
#pd.read_csv로 csv파일 불러오기
wine = pd.read_csv(io.BytesIO(myfile['와인.csv']),
                       encoding='cp949')
wine

# 훈련 세트와 테스트 세트로 나눕니다.
data = wine[ ['알콜도수','당도','산도_ph']]
target = wine['종류']

from sklearn.model_selection import train_test_split
훈련용_data , 테스트용_data , 훈련용_target , 테스트용_target = train_test_split(
    data , target , test_size=0.3 , random_state=40)

#Step 4. 그레이디언트 부스팅 알고리즘으로 분석하기
from sklearn.ensemble import GradientBoostingClassifier
gb = GradientBoostingClassifier(n_estimators=300 , random_state=40)
gb.fit(훈련용_data , 훈련용_target)
gb.score(테스트용_data , 테스트용_target)

#Step 5. 과대적합 여부 살펴보기
import numpy as np
from sklearn.model_selection import cross_validate
score = cross_validate(gb , 훈련용_data , 훈련용_target ,
                        return_train_score=True , n_jobs=-1)
print( '훈련데이터점수:',np.mean(score['train_score']) , \
      '/ 검증데이터점수:', np.mean(score['test_score']))

#XGBoost 기법
from xgboost import XGBClassifier
xgb = XGBClassifier(tree_method='hist' , random_state=40 ,\
                    early_stopping_rounds=10)
score = cross_validate(xgb , 훈련용_data , 훈련용_target ,
                        return_train_score=True , n_jobs=-1)
print( '훈련데이터점수:',np.mean(score['train_score']) ,\
      '/ 검증데이터점수:', np.mean(score['test_score']))

#Light GBM 기법
from lightgbm import LGBMClassifier
lgb = LGBMClassifier(n_estimators=200 , random_state=40)
score = cross_validate(lgb , 훈련용_data , 훈련용_target ,
                        return_train_score=True , n_jobs=-1)
print( '훈련데이터점수:',np.mean(score['train_score']) ,\
      '/ 검증데이터점수:', np.mean(score['test_score']))

# 그리드서치 활용하여 하이퍼라파미터 튜닝 - XGB
from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier
import numpy as np

parms = {
    'n_estimators' : range(1,10 , 1),
    'max_depth' : [2,4,6,8,10,12] ,
    'min_samples_leaf' : [2,4,6,8,10] ,
    'min_samples_split' : [2,4,6,8,10],
    'learning_rate' : np.arange(0.1 , 1)
}

xgb2 = XGBClassifier(tree_method='hist',n_jobs=-1 , early_stopping_rounds=5)
grid = GridSearchCV(xgb2 , param_grid = parms , cv=5)
grid.fit(훈련용_data , 훈련용_target)
print('최적의 하이퍼 파라미터값' , grid.best_params_)
print('정확도:' , grid.best_score_)

# 그리드서치 활용하여 하이퍼라파미터 튜닝 - LGB
from sklearn.model_selection import GridSearchCV
from lightgbm import LGBMClassifier
import numpy as np

parms = {
    'n_estimators' : range(1,10 , 1),
    'max_depth' : [2,4,6,8,10,12] ,
    'min_samples_leaf' : [2,4,6,8,10] ,
    'min_samples_split' : [2,4,6,8,10],
    'learning_rate' : np.arange(0.1 , 1)
}

lgb2 = XGBClassifier(tree_method='hist',n_jobs=-1 )
grid = GridSearchCV(lgb2 , param_grid = parms , cv=5)
grid.fit(훈련용_data , 훈련용_target)
print('최적의 하이퍼 파라미터값' , grid.best_params_)
print('정확도:' , grid.best_score_)

